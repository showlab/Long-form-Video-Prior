<div align="center">
<h1>Long-form Video Prior ðŸŽ¥ </h1>
<h3>Learning Long-form Video Prior via Generative Pre-Training</h3>

[Jinheng Xie](https://sierkinhane.github.io/)<sup>1</sup>&nbsp; Jiajun Feng<sup>1&#42;</sup>&nbsp; Zhaoxu Tian<sup>1&#42;</sup>&nbsp; Kevin Qinghong Lin<sup>1</sup>&nbsp; Yawen Huang<sup>2</sup> Xi Xia<sup>1</sup>&nbsp; Xu Zuo<sup>1</sup>&nbsp; Jiaqi Yang<sup>1</sup>&nbsp; Yefeng Zheng<sup>2</sup>&nbsp; [Mike Zheng Shou](https://scholar.google.com/citations?hl=zh-CN&user=h1-3lSoAAAAJ&view_op=list_works&sortby=pubdate)<sup>1</sup> 

<sup>1</sup> National University of Singapore&nbsp; <sup>2</sup> Jarvis Research Center, Tencent Youtu Lab&nbsp;

[![arXiv](https://img.shields.io/badge/arXiv-<TBD>-<COLOR>.svg)]()

</div>

<img src="docs/teaser.gif" width="1000">

### Data samples
<img src="docs/sample.jpg" width="1000">

Annotated samples (part of a storyboard) of the proposed Storyboard20K. Our dataset involves three main annotations, \emph{i.e.,} (i) character-centric (whole body keypoints and bounding boxes), (ii) film-set-centric (bounding boxes), and (iii) summative (texts) annotations. It also includes condensed (as illustrated in Fig.~\ref{fig:teaser}) or shot-by-shot descriptions.

